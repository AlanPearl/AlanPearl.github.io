<!DOCTYPE HTML>
<!--
	Dimension by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
<!-- 	This is the "Head" of the HTML document.
    It contains information that isn't displayed on the actual page, but is useful
    for the web browser when loading the page
-->
<head>
    <!-- This is the title of the page. It is the text that appears inside this pages tab in your web browser -->
    <title>Alan Pearl</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <!-- 	This links to this page's CSS, which is contained in the folder assets/css and the file main.css.
         If you want to edit the styling of this page, you should edit the file assets/css/main.css
         If you have a new CSS file you'd like to add with custom styling, you should link to it here using:
         <link rel="stylesheet" href="assets/css/my-new-css-file.css"/>
    -->
    <link rel="stylesheet" href="assets/css/main.css" />
    <!-- 	In the case that the user's browser does not support JavaScript (unlikely, but possible), the page
        will load a separate set of CSS stylings from the file assets/css/noscript.css
        Any HTML contained inside <noscript></noscript> tags will be loaded in the event that JavaScript is not
        available.
    -->
    <noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
</head>
<!-- The body is the location where your site's content will go -->
<body class="is-preload">

<!-- This "div" wraps around all of our content and just changes how things are layed out -->
<div id="wrapper">

    <!-- This is where the content that appears on page load exists -->
    <header id="header">
        <!-- This is the main content of the front page -->
        <div class="content">
            <div class="inner">
                <!-- Here is a heading where you can put your name -->
                <h1>
                    Alan Pearl
                </h1>
                <!-- 	Here is an image where you can put a picture of you.
                    You can change the width and height attributes below to change how large
                    your image is.

                    Try adding "border-radius: 50%;" to the style attribute.
                -->
                <img src="images/headshot-pgh-smoothed.png" style="width:390px; height:390px; object-fit: cover; image-rendering: auto;">
                <!-- 	Here is a paragraph where you can put your position and institution, or
                    a short line about yourself.
                -->
                <p style="margin-bottom: 0px;">Postdoctoral Researcher</p>
                <p style="margin-bottom: 0px;">Cosmological Physics and Advanced Computing (CPAC) Group</p>
                <p style="margin-bottom: 0px">Argonne National Laboratory</p>
                <!-- Here is a paragraph where you can put a link to your CV -->
                <p>
                    Contact me: <a href="mailto:alanpearl13@gmail.com">alanpearl13@gmail.com</a>
                </p>
            </div>
        </div>
        <!-- This is the navigation menu -->
        <nav>
            <!-- This element makes an "Unordered List" -->
            <ul>
                <!-- This is a "List Item" -->
                <li>
                    <!-- 	Note that this links to #about, which will move the page to wherever
                        the element with the id "about" exists.
                    -->
                    <a href="#cv" style="margin-right: -10pt">Curriculum Vitae</a>
                </li>
<!--                <li>
                    <a href="#educational">Educational Materials</a>
                </li>-->
                <li>
                    <a href="#science" style="margin-right: 25pt">Data Science</a>
                </li>
                <li>
                    <a href="#software" style="margin-right: 20pt">Software</a>
                </li>
                <li>
                    <a href="#data" style="margin-right: 5pt">Data Catalogs</a>
                </li>
                <!-- 								<li>
                                                    <a href="#community">Community</a>
                                                </li>
                                                <li>
                                                    <a href="#contact">Contact</a>
                                                </li> -->
                <!-- 	You can add another button to your navigation menu by adding
                    another List Item with a link inside it.
                -->
                <!-- <li>
                    <a href="images/Astronomy.jpg">My CV</a>
                </li> -->
            </ul>
        </nav>
    </header>

    <!-- These are the "cards" that appear when you click the buttons on the main page -->
    <div id="main">
        <!-- 	Here is the "About Me" card. Note it has the id "about", which the previous
            button links to.
        -->
        <article id="cv">
            <h2 class="major">Curriculum Vitae</h2>
            <!-- Here is an image that you can customize for this page -->
            <!-- 							<span class="image main">
                                            <img src="images/pic01.jpg" alt=""/> -->
            <!--							</span>-->
            <!-- Here are three paragraphs where you can fill out information about yourself -->
            <h6 style="margin-bottom: 10pt; font-size: 20pt">
                Links
            </h6>

            <ul>
            <li>
            <p style="margin-bottom: 10pt;"><a href="documents/Resume_AlanPearl_April2025.pdf"><b>
                View my data science resume
            </b></a> (<a href="documents/FR_Resume_AlanPearl_April2025.pdf"><b>voir mon CV fran√ßais ici</b></a>) </p>
            </li>
            <li>
            <p style="margin-bottom: 10pt;"><a href="https://linkedin.com/in/alannpearl"><b>
                View my LinkedIn Profile
            </b></a></p>
            </li>
            <li>
            <p style="margin-bottom: 10pt;"><a href="documents/CV_AlanPearl_July2023.pdf"><b>
                View my full academic CV
            </b></a></p>
            </li>
            <li>
            <p style="margin-bottom: 10pt;"><a href="https://ui.adsabs.harvard.edu/search/p_=0&q=orcid%3A0000-0001-9820-9619&sort=date%20desc%2C%20bibcode%20desc">
                <b>Publications: See my papers on ADS</b>
            </a></p>
            </li>
            </ul>

            <h6 style="margin-bottom: 10pt; font-size: 20pt">
                Education
            </h6>

            <ul>
            <li>
            <p style="margin-bottom: 0;">
                <b>University of Pittsburgh</b>
            </p>
            <p style="margin-bottom: 0;">
                06/2023 - Ph.D. Physics
            </p>
            <p style="margin-bottom: 10pt;">
                12/2018 - M.S. Physics
            </p>
            </li>


            <li><p style="margin-bottom: 0;">
                <b>Rensselaer Polytechnic Institute</b>
            </p>
            <p>
                05/2017 - B.S. Physics
            </p></li>
            </ul>

        </article>

        <article id="software">
            <h2 class="major">Software Projects</h2>
            <!--							</span>-->

            <ul>

            <li><p>
                <a href="https://diffopt.readthedocs.io/">
                diffopt</a> is a Python package that combines various tools
                and techniques for parallelizable parameter optimization,
                including gradient-based techniques powered by a parallel
                extension to Jax autodiff capabililties.
            </p></li>

            <li><p>
                <a href="https://galtab.readthedocs.io/">
                galtab</a> is a key Python package that enabled much of
                the work in my PhD thesis by pretabulating galaxy
                placeholders to improve prediction efficiency of my
                Counts-in-Cylinders estimator.
            </p></li>

            <li><p>
                <a href="https://github.com/AlanPearl/JaxTabCorr/">
                JaxTabCorr</a> is a Python package that integrates classes from
                TabCorr and halotools into a differentiable prediction
                framework made possible by Jax autodiff libraries.
            </p></li>

            <li><p>
                <a href="https://github.com/AlanPearl/mocksurvey/">
                mocksurvey</a> is a Python package used for constructing mock
                galaxy catalogs and perform mock surveys seeded from the
                UniverseMachine empirical model.
            </p></li>

            <li><p>
                I contribute to <a href="https://halotools.readthedocs.io/">
                halotools</a>, which is a Python package that provides a wide
                array of models of the galaxy-halo connection.
            </p></li>
            </ul>

        </article>
        <article id="data">
            <h2 class="major">Data Products</h2>
            <!--							</span>-->

            <h3>Mock Galaxy Catalogs</h3>
            <p>
                You can download my mock catalogs for PFS <a href="https://anl.box.com/s/r5yk4twcnagfbd93xg1npydkcsumroyj">here</a>
                (or <a href="https://pitt-my.sharepoint.com/:f:/g/personal/anp180_pitt_edu/EvmbPGjRpv9FiFl0p98QHQkBvB1e3MOPk-9eJuuOKkFFMw?e=7CE0g5">here</a>
                for the original May 2020 version).
                
            </p>
        </article>

        <!-- Here is the "Projects" card. -->
        <article id="science">
            <h2 class="major">My Data Science Projects</h2>

            <p>
                <!-- I have led the development of the following data science projects during my postdoc at Argonne National Lab. Continue scrolling to learn about the class of astrophyiscal models they are being applied to. Documentation for these projects can all be found within the 
                <a href="https://diffopt.readthedocs.io">diffopt</a> docs page. -->

                During my postdoc at Argonne National Lab, I have worked on the optimization, parallelization, and development of machine learning components of a suite of galaxy-halo connection models within the <a href="https://diffsky.readthedocs.io">diffsky</a> ecosystem. A full diffsky model takes hundreds of parameter inputs and is used to make predictions and inferences about the statistical occurrences of physical galaxy formation processes.
                <br><br>
                Below, I describe just a couple of the key projects I have completed:

                <ol style="margin-top: -25px;">
                    <li>
                        <a href="https://diffmahnet.readthedocs.io">diffmahnet</a>: Neural density estimation to generate halo mass accretion histories
                    </li>
                    <li>
                        <a href="https://diffopt.readthedocs.io">diffopt</a>: Tools for parallelizable optimization of diffsky (or any JAX model)
                    </li>
                </ol>

            </p>

            <p style="font-size:24px;"><b><a href="https://diffmahnet.readthedocs.io">DiffMahNet</a></b></p>

            <p>
                The <a href="https://diffmahnet.readthedocs.io">diffmahnet package</a> provides a framework and pretrained models for emulating <a href="https://diffmahpop.readthedocs.io">diffmahpop</a>&mdash;a crucial component of the diffsky ecosystem&mdash;which is a generative model of populations of halo mass accretion histories. I implemented this emulator using neural density estimation, which is a type of neural network that predicts distributions rather than point estimates. This is a useful technique for emulating the properties of any population that is not well-described by a simple parametric distribution. The diffmahnet package is built on top of JAX, using the <a href="https://danielward27.github.io/flowjax/">flowjax</a> library, which allows us to take advantage of GPU acceleration and automatic differentiation.

                <br><br>

                The pretrained models we provide have already shown to be very powerful, not only because they generate realistic halo mass accretion histories quicker than standard techniques, but also because they can be trivially extended to include cosmological dependence. To this end, in the near future, we will include additional training data from various simulations that span a range of cosmological parameters, so that the emulator can be used to generate realistic halo mass accretion histories for any cosmology. Since performing millions of simulations spanning many cosmologies is prohibitively expensive, diffmahnet will fundamentally enhance our ability to model the halo and galaxy populations in a wide range of cosmologies, enabling inference into the nature of dark matter and dark energy from a new class of observables.
            </p>
            <p>
                <img src="images/datascience/diffmahnet-mahs.png" style="width:550px;">
                <br> Figure 1: Halo mass accretion histories (MAHs) generated by pretrained diffmahnet models for centrals (blue) and satellites (orange). Nine sets of populations are shown, each with a fixed t<sub>obs</sub> and M<sub>h</sub>(t<sub>obs</sub>), distinguished by their ultimate values of t and M<sub>h</sub>, but demonstrating the wide variance of accretion histories that can approach the same halo mass at the same time.
            </p>

            <p style="font-size:24px;"><b><a href="https://joss.theoj.org/papers/10.21105/joss.07522">DiffOpt: Parallel optimization of Jax models</a></b></p>

            <p>
                The <a href="https://diffopt.readthedocs.io">diffopt package</a> and accompanying <a href="https://joss.theoj.org/papers/10.21105/joss.07522">JOSS publication</a> aid the diffsky team and other developers of many-parameter JAX models by providing a framework for parallelizing the optimization of these models. The package is built on top of JAX, which allows us to take advantage of GPU acceleration and automatic differentiation. The package also includes a few custom implementations of optimization algorithms, which are very powerful for both diffopt and other many-parameter differentiable models, including neural networks. There are three primary subcomponents of the diffopt package, each of which I will describe below:
            </p>


            <p style="font-size:20px;"><u>kdescent</u></p>
            <p>
                Fitting a population model with many parameters to multi-dimensional demographics is a notoriously difficult machine learning problem. <a href="https://diffopt.readthedocs.io/en/latest/kdescent/intro.html">kdescent</a> presents a flexible stochastic gradient descent solution. kdescent draws a very small, random "mini-batch" of the training data (n = 20 for this example) and constructs a kernel density estimation (KDE) of the distribution, using these 20 random kernel centers, as shown by the example below.
            </p>
            <p>
                <img src="images/datascience/twenty-random-kernels.png" style="width:550px;">
                <br> Figure 2: Each panel shows example training data, colored by a kernel weight. The kernel centers were chosen randomly from the training data. We adopt a Gaussian kernel with the Scott's rule bandwidth (stretched according to the inverse principle component transformation).
            </p>
            <p>
                <img src="images/datascience/combined-kde.png" style="width:550px;">
                <br> Figure 3: The left panel is colored by number density of the training data in small spatial bins. The right panel attempts to approximately reproduce the population distribution, while only knowing 20 points using KDE (basically just averaging the twenty kernels from above).
            </p>
            <p>
                Now imagine we are modeling this population (for testing, we will simply populate samples from a 2D multivariate normal distribution, which means we are fitting 5-parameters: 2 means, 2 variances, and 1 correlation coefficient). Thanks to the JAX implementation of kdescent, we can fit these parameters using a gradient descent algorithm, such as Adam, defining the loss as the mean-squared error of the number density within each kernel. By re-drawing our kernels at each iteration, we are able to (a) fairly probe the training data and (b) avoid getting stuck in local minima. The animation below shows this 5-parameter fit in action.
            </p>
            <p>
                <img src="images/datascience/kdescent-gaussian2d-adam.gif" style="width:550px;">
                <br> Figure 4: Animation of 200 iterations of the Adam gradient descent algorithm, using the loss function described above. The left panel shows the parameters converging to their known true values. The right panel shows the model distribution moving, stretching, and rotating until it finally converges upon the correct distribution.
            </p>

            <p style="font-size:24px;"><b>Parallel Gradients</b></p>
            <p>
                The JAX Python library allows us to create extremely powerful, fast, differentiable models without requiring excessive development time. However, efficiently calculating a gradient in parallel is not trivial. This is a huge hurdle for big-data problems, where our data must be distributed across several nodes, and prevents us from taking advantage of the full processing power of each node. Therefore, I built a framework to simplify this process, and implemented it in the <a href="https://diffopt.readthedocs.io/en/latest/multigrad/intro.html">multigrad</a> package. In brief, it works by allowing users to define functions that compute linear statistics that are summed over the data on each node. Not only are the resulting statistics computed automatically, but the chain rule is preserved by exploiting the <a href="https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html#how-it-s-made-two-foundational-autodiff-functions">vector-Jacobian product</a>, allowing us to propagate the the derivatives to any desired loss function.
            </p>
            <p>
                <img src="images/datascience/multigrad-subvolumes.png" style="width:550px;">
                <br> Figure 5: Subvolume division in which each partial gradient is computed before MPI combines everything with one simple addition reduction.
            </p>

            <p style="font-size:24px;"><b>Particle Swarm Optimization</b></p>
            <p>
                The greatest challenge posed to gradient descent is the risk of getting stuck in local minima. It is therefore necessary to start from a reasonable initial condition, either using analytic approximations or exploratory parameter scans. One of the most powerful methods to run an exploratory parameter scan is known as Particle Swarm Optimization (PSO), in which a set number of particles are initialized along a Latin Hypercube with randomized velocities and accelerate according to a cognitive weight (which pulls towards the best loss found by each particle) and a social weight (which pulls towards the best loss found by the entire swarm). I have built a PSO implementation, <a href="https://diffopt.readthedocs.io/en/latest/multiswarm/intro.html">multiswarm</a>, that hosts particles across various MPI tasks. Since PSO doesn't require gradient calculations, it is applicable to a wider range of models and has much lower memory requirements than differentiable methods, making it very fast and massively parallelizable.
            </p>
            <p>
                <img src="images/datascience/pso-ackley5d-params0-1.gif" style="width:550px;">
                <br> Figure 6: A swarm of one hundred particles converging upon the global minimum of a 5-dimensional Ackley loss function. Taking all 5 dimensions into account, there are roughly 10<sup>5</sup> local minima, yet the swarm skillfully navigates closer and closer to the global minimum, reducing the search space as the particles slowly lose their momenta, until they arrive at the correct place.
            </p>

            <h2 class="major">Astrophysics</h2>
            <!-- 							<span class="image main">
                                            <img src="images/pic02.jpg" alt=""/> -->
            <!--							</span>-->


            <p>
                <b>Publications:</b>
                <a href="https://ui.adsabs.harvard.edu/search/p_=0&q=orcid%3A0000-0001-9820-9619&sort=date%20desc%2C%20bibcode%20desc">
                    See my papers on ADS</a>
            </p>

            <p style="font-size:24px;"><b>Postdoc at Argonne</b></p>
            <p>
                I work closely with Andrew Hearin and others to develop models of the galaxy-halo connection, implemented with Python's JAX library to enable GPU acceleration and automatic differentiation. I am focusing on improving the scalability of our model to extremely large datasets by designing a framework that performs distributed parallel computation, while seamlessly preserving the advantages of JAX (powered by the data science projects listed above). I plan on utilizing this framework to make self-consistent mock observations on cosmological simulations, thereby minimizing biases in the joint inference of cosmology and galaxy formation physics.
            </p>

            <p style="font-size:24px;"><b>PhD Thesis</b></p>
            <p>
                <u><a href="https://d-scholarship.pitt.edu/45118/">
                    Illuminating and Tabulating the Galaxy-Halo Connection
                </a></u>
                <ul style="margin-top: -25px;">
                    <li>
                        <a href="https://www.youtube.com/watch?v=TB56zpMLP50&ab_channel=AlanPearl">
                            Watch thesis defense
                        </a>
                    </li>
                </ul>

            </p>

            <p>

            </p>

            <p style="margin-bottom: 0;">
                <b>Part I: Illuminated the UniverseMachine to construct PFS mock catalogs</b>
            </p>
            <p>
                Using UniverseMachine as a model and UltraVISTA photometry as training data, I created
                a mock galaxy catalog specifically tailored to making predictions for the upcoming
                PFS survey. Using this mock, I published a <a href="https://arxiv.org/abs/2112.00035">paper</a>
                which demonstrated that future extensions
                of the PFS survey should prioritize increasing the survey area to best improve scientific
                goals. This
                <a href="https://pitt.box.com/s/3mes7dipabo1mx06m4xdd5yaq0fr9461">mock</a>
                and the <a href="https://github.com/AlanPearl/mocksurvey">methods</a> used to create it are publicly available.
            </p>

            <p style="margin-bottom: 0;">
                <b>Part II: Tabulated statistical estimators to be fast, precise, and differentiable</b>
            </p>
            <p style="margin-bottom: 5pt">
                The galaxy-halo connection is typically analyzed via Markov-chain Monte Carlo (MCMC)
                sampling of parameter-space in order to place constraints on models. However, this process
                is slowed down by the stochastic nature of halo occupation distribution (HOD) models.
                I have improved the efficiency of this process with two open-source projects:
            </p>
                <ul>
                <li>
                    <a href="https://github.com/AlanPearl/JaxTabCorr">JaxTabCorr</a>, in which I
                    have rewritten parts of the TabCorr and halotools packages to replace certain
                    NumPy operations with equivalent JAX operations. It can be used to calculate
                    differentiable predictions of two-point correlation functions, which will
                    improve the scalability of model inference as we need to push to larger and
                    larger parameter spaces.
                </li>
                <li>
                    <a href="https://github.com/AlanPearl/galtab">galtab</a>, in which I have
                    implemented a tabulation-accelerated statistic called Counts-in-Cylinders
                    (CiC) that captures higher-order clustering information beyond that of the
                    two-point correlation function. This code is also differentiable and
                    automatically GPU-accelerated. I have <a
                    href="https://arxiv.org/abs/2309.08675">published a paper</a> presenting this
                    code, as well as the new HOD constraints that it has made possible, utilizing
                    the Early Data Release from the Dark Energy Spectroscopic Instrument (DESI).
                </li>
                </ul>
            <!--							<p>
                                            <b>Project II Title</b><br>Project description, including further links
                                                                    to posters, talks, publications.
                                        </p> -->
        </article>

    </div>

    <!-- This is the footer that appears at the bottom of the page -->
    <footer id="footer">
        <!-- 	You can change "Untitled" to your name to show everyone that
            this is your work.
        -->
        <p class="copyright">&copy; Alan Pearl. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
    </footer>
    <p style="margin-bottom: 8pt;"></p>
    <ul class="icons">
        <li><a href="https://github.com/AlanPearl" class="icon brands fa-github">
            <span class="label">Github</span></a></li>
        <li><a href="https://linkedin.com/in/alannpearl" class="icon brands fa-linkedin">
            <span class="label">Github</span></a></li>
    </ul>


</div>

<!--	This is the background image of the site.
    All configuration of the background is done with CSS.
    Look in the file assets/css/main.css and search for "#bg" to
    see how this element is styled. Look for comments pointing
    to where you can set a new background image.
-->
<div id="bg"></div>

<!-- Scripts -->
<script src="assets/js/jquery.min.js"></script>
<script src="assets/js/browser.min.js"></script>
<script src="assets/js/breakpoints.min.js"></script>
<script src="assets/js/util.js"></script>
<script src="assets/js/main.js"></script>

</body>
</html>
